# üéØ Semantic Vector Search Implementation Guide

**T√¨nh tr·∫°ng hi·ªán t·∫°i:** ‚úÖ ƒê√£ implement v·ªõi Local Embedding Provider (all-MiniLM-L6-v2)

> **üìò Xem h∆∞·ªõng d·∫´n chi ti·∫øt:** [LOCAL_EMBEDDING_GUIDE.md](./LOCAL_EMBEDDING_GUIDE.md)

---

## üìä T√¨nh Tr·∫°ng Hi·ªán T·∫°i

### ‚úÖ ƒê√£ Implement (Complete)

1. **PostgreSQL + pgvector:**
   - Extension `vector` ƒë√£ enable
   - Tables v·ªõi c·ªôt `embedding_vector vector(384)` ‚úÖ
   - Indexes cho vector search

2. **Database Schema:**
   ```sql
   -- recent_chapters table
   embedding_vector vector(384)  -- ‚úÖ Updated for Local Embedding
   
   -- chapter_chunks table  
   chunk_embedding vector(384)  -- ‚úÖ Updated for Local Embedding
   
   -- embedding_cache table
   embedding_vector vector(384)  -- ‚úÖ Updated for Local Embedding
   ```

3. **Services ƒê√£ T·∫°o:**
   - `semanticSearchService.ts` - Semantic search logic ‚úÖ
   - `embeddingProvider.ts` - Pluggable embedding providers ‚úÖ
   - `vertexEmbeddingService.ts` - Uses Local/OpenAI/Vertex AI ‚úÖ
   - `hybridSearchService.ts` - Hybrid search ‚úÖ
   - `embeddingCacheService.ts` - Caching ‚úÖ
   - `hierarchicalEmbeddingService.ts` - Hierarchical search ‚úÖ

### ‚úÖ Current Implementation

**Local Embedding Provider (Default - Free):**
- Model: `all-MiniLM-L6-v2` (384 dimensions)
- Cost: FREE (100% local)
- Performance: ~85% of OpenAI embeddings quality
- See: [LOCAL_EMBEDDING_GUIDE.md](./LOCAL_EMBEDDING_GUIDE.md) for complete setup

---

## üéØ M·ª•c Ti√™u

> **T√≠ch h·ª£p semantic vector search th·ª±c s·ª±** v√†o h·ªá th·ªëng, cho ph√©p t√¨m ki·∫øm n·ªôi dung "theo √Ω nghƒ©a" thay v√¨ ch·ªâ t·ª´ kh√≥a.

**‚úÖ Current Flow (Local Embedding - Default):**
```
[User Query]
    ‚Üì
[Local Embedding Provider] ‚Üí Semantic embedding (384 dims) ‚úÖ
    ‚Üì
[PostgreSQL + pgvector] ‚Üí Vector search (cosine similarity)
    ‚Üì
[Backend TypeScript] ‚Üí Tr·∫£ k·∫øt qu·∫£ semantic
```

**Alternative Flows (Paid Options):**
```
[User Query]
    ‚Üì
[OpenAI / Vertex AI] ‚Üí Semantic embedding (1536/768 dims)
    ‚Üì
[PostgreSQL + pgvector] ‚Üí Vector search (cosine similarity)
    ‚Üì
[Backend TypeScript] ‚Üí Tr·∫£ k·∫øt qu·∫£ semantic
```

> **üìò For Local Embedding setup:** See [LOCAL_EMBEDDING_GUIDE.md](./LOCAL_EMBEDDING_GUIDE.md)

---

## üîß Option 0: Local Embedding (Free - Default) ‚úÖ

**Status:** ‚úÖ Implemented and Active

> **üìò Complete guide:** [LOCAL_EMBEDDING_GUIDE.md](./LOCAL_EMBEDDING_GUIDE.md)

**Quick Setup:**
```bash
# Install Python dependencies
pip install sentence-transformers fastapi uvicorn

# Start Local Embedding Server
python local_embedding_server.py

# Configure .env
EMBEDDING_PROVIDER=local
LOCAL_EMBEDDING_API_URL=http://localhost:8000
```

**Features:**
- Model: `all-MiniLM-L6-v2` (384 dimensions)
- Cost: FREE (100% local)
- Privacy: 100% (no data sent to external APIs)
- Performance: ~85% of OpenAI embeddings quality

---

## üîß Option 1: Google Vertex AI (Paid - Alternative)

### 1.1. Setup Google Vertex AI

**File:** `server/services/vertexEmbeddingService.ts`

```typescript
import { VertexAI } from '@google-cloud/aiplatform';
import { getCachedEmbedding, cacheEmbedding } from './embeddingCacheService';

const EMBEDDING_DIMENSIONS = 768;
const MODEL_NAME = 'text-embedding-004'; // Google's embedding model

// Initialize Vertex AI client
const vertexAI = new VertexAI({
  project: process.env.GOOGLE_CLOUD_PROJECT_ID!,
  location: 'us-central1',
});

/**
 * Generate semantic embedding using Google Vertex AI
 */
export async function generateEmbedding(
  content: string,
  modelVersion: string = MODEL_NAME
): Promise<number[]> {
  try {
    // Check cache first
    const cached = await getCachedEmbedding(content, modelVersion);
    if (cached) {
      return cached;
    }
    
    // Generate embedding via Vertex AI
    const model = vertexAI.preview.getGenerativeModel({
      model: modelVersion,
    });
    
    // For embeddings, use the embedding API
    const response = await fetch(
      `https://us-central1-aiplatform.googleapis.com/v1/projects/${process.env.GOOGLE_CLOUD_PROJECT_ID}/locations/us-central1/publishers/google/models/${modelVersion}:predict`,
      {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${await getAccessToken()}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          instances: [{ content }],
        }),
      }
    );
    
    if (!response.ok) {
      throw new Error(`Vertex AI API error: ${response.statusText}`);
    }
    
    const data = await response.json();
    const embedding = data.predictions[0].embeddings.values;
    
    // Validate dimensions
    if (embedding.length !== EMBEDDING_DIMENSIONS) {
      throw new Error(`Expected ${EMBEDDING_DIMENSIONS} dimensions, got ${embedding.length}`);
    }
    
    // Cache the embedding
    await cacheEmbedding(content, embedding, modelVersion);
    
    return embedding;
  } catch (error) {
    console.error('Embedding generation error:', error);
    throw error;
  }
}

/**
 * Get access token for Vertex AI
 */
async function getAccessToken(): Promise<string> {
  // Use service account or application default credentials
  const { GoogleAuth } = require('google-auth-library');
  const auth = new GoogleAuth({
    scopes: ['https://www.googleapis.com/auth/cloud-platform'],
  });
  const client = await auth.getClient();
  const accessToken = await client.getAccessToken();
  return accessToken.token!;
}
```

### 1.2. Environment Variables

**File:** `.env`

```env
# Google Cloud
GOOGLE_CLOUD_PROJECT_ID=your-project-id
GOOGLE_APPLICATION_CREDENTIALS=path/to/service-account.json

# Or use service account email/key
GOOGLE_SERVICE_ACCOUNT_EMAIL=your-service-account@project.iam.gserviceaccount.com
GOOGLE_SERVICE_ACCOUNT_PRIVATE_KEY="-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\n"
```

### 1.3. Install Dependencies

```bash
npm install @google-cloud/aiplatform google-auth-library
```

---

## üîß Option 2: OpenAI Embeddings (Alternative)

N·∫øu mu·ªën d√πng OpenAI thay v√¨ Google Vertex AI:

### 2.1. Update Service

**File:** `server/services/openaiEmbeddingService.ts` (t·∫°o m·ªõi)

```typescript
import OpenAI from 'openai';
import { getCachedEmbedding, cacheEmbedding } from './embeddingCacheService';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

const EMBEDDING_DIMENSIONS = 1536; // OpenAI text-embedding-3-small
const MODEL_NAME = 'text-embedding-3-small';

/**
 * Generate semantic embedding using OpenAI
 */
export async function generateEmbedding(
  content: string,
  modelVersion: string = MODEL_NAME
): Promise<number[]> {
  try {
    // Check cache first
    const cached = await getCachedEmbedding(content, modelVersion);
    if (cached) {
      return cached;
    }
    
    // Generate embedding via OpenAI
    const response = await openai.embeddings.create({
      model: modelVersion,
      input: content,
    });
    
    const embedding = response.data[0].embedding;
    
    // Validate dimensions
    if (embedding.length !== EMBEDDING_DIMENSIONS) {
      throw new Error(`Expected ${EMBEDDING_DIMENSIONS} dimensions, got ${embedding.length}`);
    }
    
    // Cache the embedding
    await cacheEmbedding(content, embedding, modelVersion);
    
    return embedding;
  } catch (error) {
    console.error('OpenAI embedding generation error:', error);
    throw error;
  }
}

/**
 * Generate embeddings in batch
 */
export async function generateEmbeddingsBatch(
  contents: string[],
  modelVersion: string = MODEL_NAME
): Promise<number[][]> {
  try {
    const response = await openai.embeddings.create({
      model: modelVersion,
      input: contents,
    });
    
    return response.data.map(item => item.embedding);
  } catch (error) {
    console.error('OpenAI batch embedding error:', error);
    throw error;
  }
}
```

### 2.2. Update Database Schema

N·∫øu d√πng OpenAI (1536 dims), c·∫ßn update schema:

```sql
-- Update embedding dimensions
ALTER TABLE recent_chapters 
  ALTER COLUMN embedding_vector TYPE vector(1536);

ALTER TABLE chapter_chunks 
  ALTER COLUMN chunk_embedding TYPE vector(1536);

ALTER TABLE embedding_cache 
  ALTER COLUMN embedding_vector TYPE vector(1536);
```

### 2.3. Environment Variables

```env
OPENAI_API_KEY=sk-...
```

### 2.4. Install Dependencies

```bash
npm install openai
```

---

## üîß Option 3: Google Gemini Embeddings (Simplest - ƒê√£ c√≥ Gemini API)

N·∫øu ƒë√£ c√≥ Gemini API key, c√≥ th·ªÉ d√πng Gemini ƒë·ªÉ generate embeddings:

**File:** `server/services/geminiEmbeddingService.ts`

```typescript
import { GoogleGenerativeAI } from '@google/generative-ai';
import { getCachedEmbedding, cacheEmbedding } from './embeddingCacheService';

const genAI = new GoogleGenerativeAI(process.env.API_KEY!);
const EMBEDDING_DIMENSIONS = 768;

/**
 * Generate embedding using Gemini (via embedding API)
 * Note: Gemini doesn't have direct embedding API, but we can use it for semantic tasks
 * For actual embeddings, use Vertex AI or OpenAI
 */
export async function generateEmbedding(
  content: string
): Promise<number[]> {
  // Gemini doesn't have embedding API directly
  // Use Vertex AI embedding API instead
  // This is a placeholder - implement with Vertex AI
  throw new Error('Use Vertex AI embedding service instead');
}
```

**Recommendation:** D√πng Vertex AI (Option 1) v√¨ ƒë√£ c√≥ Google Cloud setup

---

## üìù Implementation Steps

### Step 1: Ch·ªçn Embedding Provider

- **Option A:** Google Vertex AI (recommended - ph√π h·ª£p v·ªõi h·ªá th·ªëng)
- **Option B:** OpenAI (n·∫øu mu·ªën ƒë∆°n gi·∫£n h∆°n)
- **Option C:** Cohere, HuggingFace, etc.

### Step 2: Update `vertexEmbeddingService.ts`

Thay th·∫ø placeholder code b·∫±ng implementation th·ª±c s·ª± (theo Option 1 ho·∫∑c 2)

### Step 3: Update Environment Variables

Th√™m credentials v√†o `.env`

### Step 4: Test Embedding Generation

```typescript
// Test script
import { generateEmbedding } from './server/services/vertexEmbeddingService';

(async () => {
  const embedding = await generateEmbedding("Test content");
  console.log('Embedding dimensions:', embedding.length);
  console.log('First 5 values:', embedding.slice(0, 5));
})();
```

### Step 5: Test Semantic Search

```typescript
// Test semantic search
import { semanticSearch } from './server/services/semanticSearchService';

(async () => {
  const results = await semanticSearch(
    "t√¨m ki·∫øm ng·ªØ nghƒ©a v·ªõi c∆° s·ªü d·ªØ li·ªáu",
    "book-id-here",
    5
  );
  console.log('Search results:', results);
})();
```

### Step 6: Update Processing Jobs

ƒê·∫£m b·∫£o `bookProcessingJob.ts` v√† `chapterProcessingJob.ts` s·ª≠ d·ª•ng embedding service m·ªõi

---

## üß™ Testing

### Test 1: Embedding Generation

```bash
npm run test:embedding
```

**Expected:**
- Embedding c√≥ ƒë√∫ng dimensions (768 ho·∫∑c 1536)
- Embedding kh√¥ng ph·∫£i zero vector
- Embedding ƒë∆∞·ª£c cache

### Test 2: Semantic Search

```bash
npm run test:semantic-search
```

**Expected:**
- T√¨m ƒë∆∞·ª£c documents li√™n quan (kh√¥ng ch·ªâ exact match)
- Results sorted by similarity
- Distance scores h·ª£p l√Ω

### Test 3: Performance

```bash
npm run test:search-performance
```

**Expected:**
- Query latency < 500ms
- Cache hit rate > 80%

---

## üìä So S√°nh Options

| Feature | **Local (Default)** | Vertex AI | OpenAI | Gemini |
|---------|---------------------|-----------|--------|--------|
| **Dimensions** | 384 | 768 | 1536 | N/A |
| **Cost** | ‚úÖ FREE | $$ | $$ | N/A |
| **Setup** | Easy | Medium | Easy | N/A |
| **Privacy** | ‚úÖ 100% local | ‚ùå Cloud | ‚ùå Cloud | N/A |
| **Integration** | ‚úÖ Ready | ‚úÖ Ph√π h·ª£p | ‚ö†Ô∏è C·∫ßn update schema | ‚ùå Kh√¥ng c√≥ embedding API |
| **Multilingual** | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **Performance** | ‚≠ê‚≠ê‚≠ê (85%) | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | N/A |

**Recommendation:** 
- **Default:** **Local (Free)** - Best for privacy, cost, and ease of setup
- **Alternative:** **Vertex AI** - If you need higher quality and have Google Cloud setup
- **Alternative:** **OpenAI** - If you need best quality and don't mind cost

---

## ‚úÖ Checklist Implementation

### **Local Embedding (Default - Free)** ‚úÖ
- [x] Ch·ªçn embedding provider: Local (all-MiniLM-L6-v2)
- [x] Update `embeddingProvider.ts` v·ªõi LocalEmbeddingProvider
- [x] Update database schema: `vector(384)`
- [x] Add environment variables
- [x] Install Python dependencies
- [x] Test embedding generation
- [x] Test semantic search
- [x] Update processing jobs
- [x] Documentation: [LOCAL_EMBEDDING_GUIDE.md](./LOCAL_EMBEDDING_GUIDE.md)

### **Alternative Providers (Optional)**
- [ ] Ch·ªçn embedding provider (Vertex AI / OpenAI)
- [ ] Update `.env` v·ªõi provider credentials
- [ ] Update database schema n·∫øu c·∫ßn (vector dimensions)
- [ ] Test embedding generation
- [ ] Test semantic search

---

## üîó T√†i Li·ªáu Li√™n Quan

- **[LOCAL_EMBEDDING_GUIDE.md](./LOCAL_EMBEDDING_GUIDE.md)** - ‚úÖ Complete guide for Local Embedding (Default)
- [PROJECT_STATUS.md](./PROJECT_STATUS.md) - T√¨nh tr·∫°ng d·ª± √°n
- [QUICK_START.md](./QUICK_START.md) - Quick start guide
- [server/services/semanticSearchService.ts](./server/services/semanticSearchService.ts) - Search service
- [server/services/embeddingProvider.ts](./server/services/embeddingProvider.ts) - Embedding provider (pluggable)

---

## üí° Next Steps

1. **‚úÖ Complete:** Local Embedding Provider implemented
2. **Optional:** Switch to Vertex AI or OpenAI if needed (change `.env`)
3. **Short-term:** Test v√† optimize semantic search
4. **Long-term:** Add hybrid search (semantic + keyword), improve caching

---

**Status:** ‚úÖ Implementation Complete (Local Embedding)  
**Default Provider:** Local (Free) - `all-MiniLM-L6-v2` (384 dimensions)  
**Alternative:** Vertex AI / OpenAI (Paid) - Switch via `.env`

