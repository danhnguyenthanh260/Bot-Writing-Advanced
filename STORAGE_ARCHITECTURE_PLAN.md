# Storage Architecture Plan - Book Context & Workspace Management

## üìã T·ªïng Quan Plan C·ªßa B·∫°n

B·∫°n mu·ªën c√≥ 3 lo·∫°i storage v·ªõi PostgreSQL:

1. **Book Context Storage** - L∆∞u th√¥ng tin quan tr·ªçng v·ªÅ to√†n b·ªô book (whole thing)
2. **Visualization/Specification Storage** - L∆∞u chi ti·∫øt cho 5 chapters g·∫ßn nh·∫•t
3. **Workspace Storage** - L∆∞u workspace state ƒë·ªÉ persist khi user quay l·∫°i

## üí≠ Ph√¢n T√≠ch & ƒê√°nh Gi√° Plan

### ‚úÖ ƒêi·ªÉm M·∫°nh

1. **Separation of Concerns** - T√°ch bi·ªát r√µ r√†ng gi·ªØa:
   - Deep context (whole book) vs. Recent context (5 chapters)
   - Book data vs. Workspace state

2. **Performance Optimization** - 
   - Storage 1: Lightweight, query nhanh cho to√†n b·ªô book
   - Storage 2: Focus 5 chapters g·∫ßn nh·∫•t ‚Üí reduce memory/compute cho AI agent

3. **Scalability** - 
   - Storage 1 kh√¥ng tƒÉng tr∆∞·ªüng qu√° nhi·ªÅu khi book d√†i
   - Storage 2 ch·ªâ gi·ªØ recent chapters ‚Üí manageable size

### ü§î ƒêi·ªÉm C·∫ßn Xem X√©t

1. **Data Consistency** - 
   - L√†m sao ƒë·∫£m b·∫£o Storage 1 v√† Storage 2 sync?
   - Khi chapter m·ªõi ƒë∆∞·ª£c th√™m, l√†m sao update Storage 2 (rolling window c·ªßa 5 chapters)?

2. **Query Strategy** - 
   - Khi n√†o query Storage 1 vs. Storage 2?
   - Agent c·∫ßn access c·∫£ 2 storage kh√¥ng?

3. **Embedding Strategy** - 
   - C√≥ c·∫ßn embeddings cho c·∫£ 2 storage?
   - Hay ch·ªâ Storage 2 (5 chapters) c·∫ßn embeddings cho semantic search?

## üèóÔ∏è ƒê·ªÅ Xu·∫•t Architecture

### Storage 1: Book Context (Lightweight, Whole Book)

**M·ª•c ƒë√≠ch:** 
- Store "essential DNA" c·ªßa book
- D√πng cho initial understanding, overview, book-level decisions
- Kh√¥ng c·∫ßn embeddings (too expensive, not needed)

**Data Structure:**
```
book_context:
- book_id (PK)
- title, author, genre, theme
- summary (t√≥m t·∫Øt book, ~500-1000 words)
- characters (JSON: name, role, description, relationships)
- world_setting (JSON: locations, rules, timeline)
- writing_style (JSON: tone, POV, voice characteristics)
- story_arc (JSON: act1_summary, act2_summary, act3_summary)
- metadata (word_count, chapter_count, last_updated)
- embedding? ‚Üí KH√îNG, qu√° expensive cho to√†n b·ªô book
```

**Normalization Strategy:**
- **Text Summarization** - D√πng LLM ƒë·ªÉ extract:
  - Summary t·ª´ full text ‚Üí 500-1000 words
  - Characters ‚Üí structured JSON
  - World building ‚Üí structured JSON
  - Story arc ‚Üí structured JSON
  
- **Key Extraction** - Ch·ªâ l∆∞u nh·ªØng g√¨ **quan tr·ªçng cho writing decisions:**
  - Character names, roles, relationships
  - Important locations, world rules
  - Writing style patterns
  - Story structure overview

**Query Pattern:**
- Get book context for initial understanding
- Fast lookup by book_id
- No semantic search needed (structured data)

### Storage 2: Recent Chapters (Detailed, 5 Chapters Window)

**M·ª•c ƒë√≠ch:**
- Store chi ti·∫øt c·ªßa 5 chapters g·∫ßn nh·∫•t
- D√πng cho immediate writing context
- Semantic search v·ªõi embeddings cho agent assistance

**Data Structure:**
```
recent_chapters:
- chapter_id (PK)
- book_id (FK)
- chapter_number (integer)
- title
- content (full text c·ªßa chapter)
- summary (~200 words)
- key_scenes (JSON: scene descriptions)
- character_appearances (JSON: characters in chapter)
- plot_points (JSON: events, conflicts, resolutions)
- writing_notes (JSON: author notes, AI suggestions)
- embedding_vector (vector, 768 or 1536 dimensions)
- created_at, updated_at
- window_position (1-5, rolling window)
```

**Normalization Strategy:**
- **Full Text Storage** - Gi·ªØ nguy√™n content c·ªßa 5 chapters g·∫ßn nh·∫•t
- **Structured Metadata** - Extract:
  - Key scenes
  - Character appearances
  - Plot points
  - Writing notes
  
- **Embedding Generation** - 
  - Generate embeddings cho full chapter content
  - D√πng Vertex AI embeddings (text-embedding-004 ho·∫∑c textembedding-gecko@003)
  - Store vector trong PostgreSQL v·ªõi pgvector extension

**Query Pattern:**
- Get 5 most recent chapters by book_id
- Semantic search trong 5 chapters (d√πng vector similarity)
- Update rolling window khi c√≥ chapter m·ªõi

**Rolling Window Logic:**
```sql
-- Khi chapter m·ªõi ƒë∆∞·ª£c th√™m:
-- 1. Delete chapter v·ªõi window_position = 5 (oldest)
-- 2. Shift window_position: 4‚Üí5, 3‚Üí4, 2‚Üí3, 1‚Üí2
-- 3. Insert new chapter v·ªõi window_position = 1
```

### Storage 3: Workspace State

**M·ª•c ƒë√≠ch:**
- Persist workspace state khi user quay l·∫°i
- Store user preferences, UI state, project selection

**Data Structure:**
```
workspaces:
- workspace_id (PK)
- user_id (FK to users)
- name
- selected_book_id
- selected_chapter_id
- canvas_state (JSON: pages positions, sizes, zoom)
- chat_messages (JSON: recent messages, max 50)
- settings (JSON: theme, preferences)
- created_at, updated_at
- last_accessed_at

workspace_projects:
- project_id (PK)
- workspace_id (FK)
- book_id (FK)
- project_name
- project_state (JSON: current phase, notes)
- created_at, updated_at
```

**Normalization Strategy:**
- **JSON Storage** - Store UI state, messages, settings as JSON
- **Reference Data** - Link to books, chapters, users
- **Soft Delete** - Mark deleted, kh√¥ng hard delete

**Query Pattern:**
- Load workspace by user_id
- Update workspace state on changes
- Cleanup old messages (keep last 50)

## üîÑ Data Flow & Normalization Process

### Input: Raw Google Docs Content

**Raw Data:**
```
{
  doc_id: "...",
  title: "...",
  plain_text: "...", // Full book text
  outline: [...],    // Chapters structure
  word_count: 10000
}
```

### Step 1: Book-Level Analysis (Storage 1)

**Process:**
1. **Send to LLM** (Gemini) v·ªõi prompt:
   ```
   Analyze this book and extract:
   - Summary (500-1000 words)
   - Characters (names, roles, relationships)
   - World setting (locations, rules)
   - Writing style (tone, POV, voice)
   - Story arc (3-act structure)
   ```

2. **Parse LLM Response** ‚Üí Structured JSON

3. **Store in book_context table**

**Normalization Techniques:**
- **Summarization** - D√πng LLM ƒë·ªÉ compress full text ‚Üí summary
- **Structured Extraction** - D√πng LLM v·ªõi structured output (JSON schema)
- **Key Information Only** - Ch·ªâ l∆∞u nh·ªØng g√¨ ·∫£nh h∆∞·ªüng ƒë·∫øn writing decisions

### Step 2: Chapter-Level Processing (Storage 2)

**Process:**
1. **Split by Chapters** - D·ª±a v√†o outline ho·∫∑c chapter markers

2. **For Each Chapter (current + 4 previous):**
   - **Full Text** ‚Üí Store trong `content`
   - **Generate Summary** ‚Üí LLM summary (~200 words)
   - **Extract Metadata** ‚Üí LLM extract:
     - Key scenes
     - Character appearances
     - Plot points
   
3. **Generate Embeddings:**
   - **D√πng Vertex AI Embeddings API**
   - Input: Full chapter content ho·∫∑c summary (t√πy accuracy/speed tradeoff)
   - Output: Vector (768 ho·∫∑c 1536 dimensions)
   - Store trong `embedding_vector` column

4. **Store in recent_chapters** v·ªõi `window_position`

**Normalization Techniques:**
- **Chunking** - N·∫øu chapter qu√° d√†i (>2000 words), chunk v√† embed ri√™ng
- **Hybrid Embedding** - 
  - Full chapter embedding (for semantic search)
  - Scene-level embeddings (for fine-grained search)
- **Metadata Extraction** - Structured JSON t·ª´ LLM

### Step 3: Workspace Persistence

**Process:**
1. **User Interactions** ‚Üí Update workspace state
2. **Save to workspace table** on changes
3. **Load from workspace table** on login

## ü§ñ Vertex AI Embeddings - N√™n D√πng Kh√¥ng?

### ‚úÖ N√™n D√πng Cho Storage 2 (Recent Chapters)

**L√Ω do:**
1. **Semantic Search** - Agent c·∫ßn t√¨m "similar scenes" ho·∫∑c "context about character X"
2. **Manageable Size** - Ch·ªâ 5 chapters ‚Üí reasonable embedding cost
3. **Performance** - Vector similarity search r·∫•t nhanh v·ªõi pgvector
4. **Quality** - Vertex AI embeddings t·ªët cho Vietnamese text

**Recommendation:**
- **Model:** `text-embedding-004` ho·∫∑c `textembedding-gecko@003`
- **Dimensions:** 768 ho·∫∑c 1536 (t√πy accuracy/speed tradeoff)
- **Input:** Chapter content ho·∫∑c chapter summary
- **Storage:** PostgreSQL v·ªõi `pgvector` extension

### ‚ùå KH√îNG N√™n Cho Storage 1 (Book Context)

**L√Ω do:**
1. **Structured Data** - Book context l√† JSON structured, kh√¥ng c·∫ßn semantic search
2. **Query Pattern** - Lookup by book_id, kh√¥ng c·∫ßn similarity search
3. **Cost** - Embedding to√†n b·ªô book content qu√° expensive
4. **Unnecessary** - Key-value lookup ƒë·ªß cho use case

## üìä Database Schema Proposal

### Schema Structure

```sql
-- Books table (core)
books (
  book_id UUID PRIMARY KEY,
  google_doc_id TEXT UNIQUE,
  title TEXT NOT NULL,
  author TEXT,
  total_word_count INTEGER,
  total_chapters INTEGER,
  created_at TIMESTAMP,
  updated_at TIMESTAMP
);

-- Storage 1: Book Context (lightweight)
book_contexts (
  book_id UUID PRIMARY KEY REFERENCES books(book_id),
  summary TEXT,  -- ~500-1000 words
  characters JSONB,  -- {name, role, description, relationships}
  world_setting JSONB,  -- {locations, rules, timeline}
  writing_style JSONB,  -- {tone, pov, voice}
  story_arc JSONB,  -- {act1, act2, act3}
  metadata JSONB,
  created_at TIMESTAMP,
  updated_at TIMESTAMP
);

-- Storage 2: Recent Chapters (detailed + embeddings)
recent_chapters (
  chapter_id UUID PRIMARY KEY,
  book_id UUID REFERENCES books(book_id),
  chapter_number INTEGER NOT NULL,
  title TEXT,
  content TEXT NOT NULL,  -- Full chapter text
  summary TEXT,  -- ~200 words
  key_scenes JSONB,
  character_appearances JSONB,
  plot_points JSONB,
  writing_notes JSONB,
  embedding_vector vector(768),  -- pgvector
  window_position INTEGER CHECK (window_position BETWEEN 1 AND 5),
  created_at TIMESTAMP,
  updated_at TIMESTAMP,
  UNIQUE(book_id, window_position)
);

-- Index for semantic search
CREATE INDEX ON recent_chapters USING ivfflat (embedding_vector vector_cosine_ops);

-- Storage 3: Workspaces
workspaces (
  workspace_id UUID PRIMARY KEY,
  user_id UUID REFERENCES users(user_id),
  name TEXT,
  selected_book_id UUID REFERENCES books(book_id),
  selected_chapter_id UUID REFERENCES recent_chapters(chapter_id),
  canvas_state JSONB,  -- {pages: [...], zoom, pan}
  chat_messages JSONB,  -- [{id, role, text, timestamp}, ...]
  settings JSONB,
  created_at TIMESTAMP,
  updated_at TIMESTAMP,
  last_accessed_at TIMESTAMP
);
```

## üîÑ Normalization Workflow

### Workflow 1: New Book Imported

1. **Extract from Google Docs:**
   - Full text
   - Outline (chapters)
   - Metadata (title, word count)

2. **Generate Book Context (Storage 1):**
   - Send to LLM: "Analyze this book and extract..."
   - Parse structured response
   - Save to `book_contexts`

3. **Process Recent Chapters (Storage 2):**
   - Split into chapters
   - For last 5 chapters (ho·∫∑c t·∫•t c·∫£ n·∫øu < 5):
     - Generate summary
     - Extract metadata
     - Generate embeddings (Vertex AI)
     - Save to `recent_chapters` with window_position

4. **Create/Update Workspace:**
   - Create workspace entry
   - Set selected_book_id
   - Initialize canvas_state

### Workflow 2: New Chapter Added

1. **Process New Chapter:**
   - Full text ‚Üí `content`
   - Generate summary
   - Extract metadata
   - Generate embedding

2. **Update Rolling Window:**
   - Delete chapter v·ªõi `window_position = 5`
   - Shift positions: 4‚Üí5, 3‚Üí4, 2‚Üí3, 1‚Üí2
   - Insert new chapter v·ªõi `window_position = 1`

3. **Update Book Context (if needed):**
   - Update `total_chapters`
   - Optionally: Update summary n·∫øu chapter quan tr·ªçng

### Workflow 3: Agent Query (Writing Assistance)

1. **Get Book Context (Storage 1):**
   - Query `book_contexts` by book_id
   - Get: summary, characters, world_setting, writing_style

2. **Get Recent Context (Storage 2):**
   - Query `recent_chapters` by book_id
   - Get: 5 most recent chapters v·ªõi full content

3. **Semantic Search (if needed):**
   - User query ‚Üí Generate embedding
   - Vector similarity search trong `recent_chapters.embedding_vector`
   - Return relevant chapters/scenes

4. **Combine Context:**
   - Book-level context (Storage 1)
   - Recent chapters context (Storage 2)
   - Semantic search results (n·∫øu c√≥)
   - Send to LLM for writing assistance

## üí° Recommendations

### 1. Storage Strategy

‚úÖ **T·ªët:**
- Separate lightweight (Storage 1) vs. detailed (Storage 2)
- Rolling window cho recent chapters (Storage 2)
- Workspace persistence (Storage 3)

üí° **ƒê·ªÅ xu·∫•t th√™m:**
- **Cache Layer** - Redis cache cho frequent queries
- **Archive Storage** - Move old chapters t·ª´ Storage 2 sang archive table

### 2. Embedding Strategy

‚úÖ **D√πng Vertex AI Embeddings:**
- **Cho Storage 2** (recent chapters)
- Model: `text-embedding-004` ho·∫∑c `textembedding-gecko@003`
- **KH√îNG d√πng cho Storage 1** (unnecessary, too expensive)

üí° **Optimization:**
- **Chunk long chapters** - If chapter > 2000 words, split v√† embed chunks
- **Hybrid search** - Combine vector similarity v·ªõi keyword search
- **Cache embeddings** - Don't regenerate if chapter unchanged

### 3. Normalization Strategy

‚úÖ **T√°ch bi·ªát extraction layers:**
- **Level 1:** Raw extraction (Google Docs ‚Üí structured data)
- **Level 2:** LLM analysis (full text ‚Üí summary, metadata)
- **Level 3:** Embedding generation (content ‚Üí vectors)

üí° **Best Practices:**
- **Batch processing** - Process multiple chapters c√πng l√∫c
- **Incremental updates** - Ch·ªâ update khi c√≥ changes
- **Validation** - Validate LLM output tr∆∞·ªõc khi save
- **Fallback** - N·∫øu LLM fails, keep raw data

### 4. Query Performance

‚úÖ **Indexes:**
- `book_id` indexes tr√™n c·∫£ 3 storage
- Vector index cho `embedding_vector`
- `window_position` index cho rolling window queries

üí° **Query optimization:**
- **Materialized views** - Pre-compute frequent queries
- **Partitioning** - Partition chapters by book_id n·∫øu nhi·ªÅu data

## üéØ Final Recommendations

### Architecture Decision

1. **Storage 1 (Book Context):**
   - ‚úÖ Structured JSON, no embeddings
   - ‚úÖ LLM extraction cho normalization
   - ‚úÖ Fast lookup by book_id

2. **Storage 2 (Recent Chapters):**
   - ‚úÖ Full text + structured metadata
   - ‚úÖ **Vertex AI embeddings** cho semantic search
   - ‚úÖ Rolling window (5 chapters)
   - ‚úÖ pgvector extension

3. **Storage 3 (Workspace):**
   - ‚úÖ JSON state storage
   - ‚úÖ Soft delete
   - ‚úÖ Automatic save/load

### Normalization Pipeline

```
Google Docs Input
    ‚Üì
[Extract Raw Data]
    ‚Üì
    ‚îú‚îÄ‚Üí [LLM Analysis] ‚Üí Storage 1 (Book Context)
    ‚îÇ      (Summary, Characters, World, Style, Arc)
    ‚îÇ
    ‚îî‚îÄ‚Üí [Split Chapters] ‚Üí [Process Each Chapter]
                              ‚îú‚îÄ‚Üí Generate Summary (LLM)
                              ‚îú‚îÄ‚Üí Extract Metadata (LLM)
                              ‚îî‚îÄ‚Üí Generate Embedding (Vertex AI)
                                   ‚Üí Storage 2 (Recent Chapters)
```

### Cost Optimization

1. **Embeddings:**
   - Ch·ªâ generate cho Storage 2 (5 chapters)
   - Cache embeddings (don't regenerate if unchanged)
   - Batch processing ƒë·ªÉ reduce API calls

2. **LLM Calls:**
   - Batch analysis cho multiple chapters
   - Cache summaries n·∫øu chapter unchanged
   - Use cheaper models cho simple extraction

3. **Storage:**
   - Archive old chapters (move out of Storage 2)
   - Compress JSONB fields n·∫øu l·ªõn
   - Index optimization

## ‚ùì Questions to Consider

1. **How to handle very long chapters?**
   - Chunk v√† embed separately?
   - Or embed full chapter?

2. **Update frequency?**
   - When to regenerate book context?
   - When to update embeddings?

3. **Multiple books per user?**
   - How to handle workspace v·ªõi multiple books?

4. **Chapter ordering?**
   - How to determine "5 most recent"? By chapter_number or by updated_at?

---

*ƒê√¢y l√† high-level architecture plan. B·∫°n c√≥ mu·ªën t√¥i chi ti·∫øt h√≥a ph·∫ßn n√†o kh√¥ng?*






