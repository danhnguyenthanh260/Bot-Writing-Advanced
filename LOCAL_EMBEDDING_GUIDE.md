# ğŸ“˜ Local Embedding Guide: Flow HoÃ n Chá»‰nh & Setup

**Embedding Provider:** Local (Free) - `all-MiniLM-L6-v2` (384 dimensions)  
**Database:** PostgreSQL + pgvector vá»›i `vector(384)`  
**Architecture:** Free-first, pluggable, cÃ³ thá»ƒ switch provider sau

---

## ğŸ“‹ Má»¥c Lá»¥c

1. [Configuration](#configuration)
2. [Setup Local Embedding Server](#setup-local-embedding-server)
3. [Flow 1: Import Processing](#flow-1-import-processing)
4. [Flow 2: Chat Processing](#flow-2-chat-processing)
5. [Files Modified](#files-modified)
6. [Testing Checklist](#testing-checklist)
7. [Troubleshooting](#troubleshooting)

---

## ğŸ¯ Configuration

### **Environment Variables**

**File:** `.env`

```env
# Embedding Provider - Local (Free)
EMBEDDING_PROVIDER=local
LOCAL_EMBEDDING_API_URL=http://localhost:8000
LOCAL_EMBEDDING_MODEL=all-MiniLM-L6-v2
LOCAL_EMBEDDING_DIMENSIONS=384

# Database
DATABASE_URL=postgresql://postgres:password@localhost:5432/bot_writing_advanced

# Google Docs API
GOOGLE_SERVICE_ACCOUNT_EMAIL=xxx@project.iam.gserviceaccount.com
GOOGLE_SERVICE_ACCOUNT_PRIVATE_KEY="-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\n"

# Gemini API (cho AI chat)
API_KEY=your_gemini_api_key
```

### **Database Schema**

**File:** `server/db/schema.sql`

```sql
-- âœ… Äáº£m báº£o vector dimensions = 384 (cho all-MiniLM-L6-v2)
ALTER TABLE recent_chapters 
  ALTER COLUMN embedding_vector TYPE vector(384);

ALTER TABLE chapter_chunks 
  ALTER COLUMN chunk_embedding TYPE vector(384);

ALTER TABLE embedding_cache 
  ALTER COLUMN embedding_vector TYPE vector(384);
```

**Migration Script:** `server/db/migrate_to_384.sql` (náº¿u database Ä‘Ã£ cÃ³ vector(768))

---

## ğŸš€ Setup Local Embedding Server

### **Step 1: Install Dependencies**

```bash
pip install sentence-transformers fastapi uvicorn
```

**LÆ°u Ã½:** Láº§n Ä‘áº§u cháº¡y sáº½ download model `all-MiniLM-L6-v2` (~80MB), cÃ³ thá»ƒ máº¥t vÃ i phÃºt.

### **Step 2: Start Server**

**Option 1: Python script**
```bash
python local_embedding_server.py
```

**Option 2: npm script**
```bash
npm run embedding:start
```

**Option 3: uvicorn**
```bash
uvicorn local_embedding_server:app --host 0.0.0.0 --port 8000
```

**Option 4: Start all services**
```bash
npm run dev:all  # Start embedding + server + frontend
```

**Expected output:**
```
Loading embedding model: all-MiniLM-L6-v2...
âœ… Model loaded: all-MiniLM-L6-v2 (384 dimensions)
INFO:     Uvicorn running on http://0.0.0.0:8000
```

### **Step 3: Test Server**

```bash
# Health check
curl http://localhost:8000/

# Test embedding
curl -X POST http://localhost:8000/embed \
  -H "Content-Type: application/json" \
  -d '{"text": "Hello world"}'
```

### **Available Models**

| Model | Dimensions | Speed | Quality | Size |
|-------|-----------|-------|---------|------|
| `all-MiniLM-L6-v2` | 384 | âš¡âš¡âš¡ | â­â­â­ | ~80MB |
| `paraphrase-MiniLM-L3-v2` | 256 | âš¡âš¡âš¡âš¡ | â­â­ | ~60MB |
| `all-mpnet-base-v2` | 768 | âš¡âš¡ | â­â­â­â­ | ~420MB |

**Recommendation:** `all-MiniLM-L6-v2` (best balance)

---

## ğŸ”„ Flow 1: Import Processing

### **Step 1: User Upload Google Doc**

**File:** `components/UploadDocForm.tsx`

User nháº­p Google Doc URL â†’ POST `/api/google-docs/ingest`

### **Step 2: Backend - Google Docs API & Database Init**

**File:** `server/routes/googleDocs.ts`

**Flow:**
1. Láº¥y document tá»« Google Docs API
2. Convert thÃ nh workProfile (frontend format)
3. Táº¡o/Cáº­p nháº­t book trong database (`books` table)
4. Táº¡o chapter records (`recent_chapters` table) - raw content, chÆ°a process
5. Queue processing jobs (async - background)
6. Tráº£ vá» response vá»›i `book_id` vÃ  processing status

**Output:**
```typescript
{
  docId: "abc123",
  document: StructuredGoogleDoc,
  workProfile: WorkProfile,
  book_id: "uuid-here", // âœ… Database book_id
  processing: {
    book_job_id: "book-uuid",
    chapter_job_ids: ["chapter-1-uuid", "chapter-2-uuid", ...],
    status: "processing"
  }
}
```

### **Step 3: Background Processing - Book Processing**

**File:** `server/jobs/bookProcessingJob.ts`

**Flow:**
1. Extract book context (summary, characters, writing style, story arc) báº±ng Gemini API
2. LÆ°u vÃ o `book_contexts` table

### **Step 4: Background Processing - Chapter Processing**

**File:** `server/jobs/chapterProcessingJob.ts`

**Flow:**
1. Detect content changes (hash comparison)
2. Extract chapter metadata (summary, key scenes, characters, plot points) báº±ng Gemini API
3. Generate hierarchical embeddings:
   - **Chapter-level embedding:** Tá»« summary â†’ Local Embedding (384 dims)
   - **Chunk-level embeddings:** Náº¿u chapter dÃ i â†’ Chunk content â†’ Local Embedding (384 dims)
4. LÆ°u embeddings vÃ o database:
   - `recent_chapters.embedding_vector` (chapter-level)
   - `chapter_chunks.chunk_embedding` (chunk-level)

**Model:** `all-MiniLM-L6-v2` (Local, 384 dimensions)

### **Step 5: Local Embedding Generation**

**File:** `server/services/embeddingProvider.ts` â†’ `LocalEmbeddingProvider`

**Flow:**
1. Backend gá»i `generateEmbedding(text)`
2. `LocalEmbeddingProvider` gá»i `http://localhost:8000/embed`
3. FastAPI server (Sentence Transformers) generate embedding
4. Return embedding vector: `[0.123, -0.456, 0.789, ...]` (384 numbers)

**Server:** `local_embedding_server.py` (FastAPI + Sentence Transformers)

### **Step 6: Save Embeddings to Database**

**File:** `server/services/hierarchicalEmbeddingService.ts`

**Flow:**
1. Convert embedding array â†’ PostgreSQL vector string: `[0.123, -0.456, ...]`
2. Save to `recent_chapters.embedding_vector` (vector(384))
3. Save to `chapter_chunks.chunk_embedding` (vector(384))

### **Step 7: Frontend - Handle Response**

**File:** `App.tsx`

**Flow:**
1. Nháº­n response tá»« `/api/google-docs/ingest`
2. LÆ°u `bookId` vÃ o `workProfile.bookId`
3. (Optional) Poll processing status

**Output:** `workProfile` cÃ³ `bookId` â†’ Bridge giá»¯a Flow 1 vÃ  Flow 2

---

## ğŸ’¬ Flow 2: Chat Processing

### **Step 1: User Query**

**File:** `App.tsx` â†’ `handleSendMessage()`

**Flow:**
1. User gÃµ message trong chat
2. Láº¥y `bookId` tá»« `workProfile.bookId` (Ä‘Æ°á»£c set tá»« Flow 1)
3. Pass `bookId` trong `documentContext`

### **Step 2: Retrieve Context tá»« Database**

**File:** `server/services/contextRetrievalService.ts` â†’ `getContextForQuery()`

**Flow:**
1. Classify query type (BOOK_LEVEL | CHAPTER_LEVEL | MIXED)
2. Get book context tá»« `book_contexts` table
3. Get recent chapters tá»« `recent_chapters` table (top 5)
4. **Semantic search** (vector search) â†’ Top 5 relevant chapters

**Output:**
```typescript
{
  book_context: BookContext,
  recent_chapters: ChapterContext[],
  semantic_results: SearchResult[] // âœ… Tá»« vector search
}
```

### **Step 3: Semantic Search vá»›i Local Embedding**

**File:** `server/services/semanticSearchService.ts` â†’ `semanticSearch()`

**Flow:**
1. Generate query embedding:
   - Gá»i `generateEmbedding(query)` â†’ Local Embedding Provider
   - Model: `all-MiniLM-L6-v2`
   - Dimensions: 384
   - Result: `[0.123, -0.456, 0.789, ...]` (384 numbers)
2. Convert sang PostgreSQL vector string: `[0.123, -0.456, ...]`
3. Vector search trong PostgreSQL:
   ```sql
   SELECT * FROM recent_chapters
   WHERE book_id = $1
     AND embedding_vector IS NOT NULL
   ORDER BY embedding_vector <=> $2::vector(384)  -- Cosine distance
   LIMIT 5
   ```
4. Return top 5 chapters (sorted by similarity)

**VÃ­ dá»¥:**
- Query: "nhÃ¢n váº­t chÃ­nh lÃ m gÃ¬?"
- Embedding: `[0.123, -0.456, ...]` (384 dims)
- Search tÃ¬m chapters cÃ³ embedding gáº§n vá»›i query embedding
- TÃ¬m Ä‘Æ°á»£c chapters nÃ³i vá» "protagonist", "main character", "hero"
- **KhÃ´ng cáº§n exact match keywords!**

### **Step 4: Construct Prompt vá»›i Database Context**

**File:** `services/geminiService.ts` â†’ `generateResponse()`

**Flow:**
1. Láº¥y context tá»« database (Step 2)
2. Build prompt vá»›i:
   - Book context (summary, characters, writing style)
   - Recent chapters (top 5)
   - Semantic search results (top 5 relevant chapters)
3. Combine vá»›i user query

**Output:** Prompt Ä‘áº§y Ä‘á»§ vá»›i context tá»« database

### **Step 5: AI Response**

**File:** `services/geminiService.ts`

**Flow:**
1. Gá»i Gemini API vá»›i prompt Ä‘áº§y Ä‘á»§
2. AI phÃ¢n tÃ­ch context vÃ  generate response
3. Return response cho user

---

## ğŸ“Š Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FLOW 1: IMPORT PROCESSING                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. User upload Google Doc URL                                â”‚
â”‚    â†“                                                          â”‚
â”‚ 2. Google Docs API â†’ Extract content                         â”‚
â”‚    â†“                                                          â”‚
â”‚ 3. Create book + chapters in database                       â”‚
â”‚    â†“                                                          â”‚
â”‚ 4. Queue processing jobs (async)                             â”‚
â”‚    â†“                                                          â”‚
â”‚ 5. Background: Extract metadata (Gemini API)                 â”‚
â”‚    â†“                                                          â”‚
â”‚ 6. Background: Generate embeddings (Local Embedding)          â”‚
â”‚    â”œâ”€ Chapter-level: summary â†’ embedding                    â”‚
â”‚    â””â”€ Chunk-level: content chunks â†’ embeddings              â”‚
â”‚    â†“                                                          â”‚
â”‚ 7. Save embeddings to database (vector(384))                 â”‚
â”‚    â†“                                                          â”‚
â”‚ âœ… Database ready for semantic search                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FLOW 2: CHAT PROCESSING                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. User query                                                â”‚
â”‚    â†“                                                          â”‚
â”‚ 2. Get bookId from workProfile                               â”‚
â”‚    â†“                                                          â”‚
â”‚ 3. Retrieve context from database                            â”‚
â”‚    â”œâ”€ Book context (book_contexts)                          â”‚
â”‚    â”œâ”€ Recent chapters (recent_chapters)                      â”‚
â”‚    â””â”€ Semantic search (vector search)                       â”‚
â”‚       â”œâ”€ Generate query embedding (Local)                   â”‚
â”‚       â”œâ”€ Vector search in PostgreSQL                        â”‚
â”‚       â””â”€ Return top 5 relevant chapters                     â”‚
â”‚    â†“                                                          â”‚
â”‚ 4. Construct prompt with context                             â”‚
â”‚    â†“                                                          â”‚
â”‚ 5. Call Gemini API                                          â”‚
â”‚    â†“                                                          â”‚
â”‚ 6. Return AI response                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”— Bridge: book_id

**Flow 1 táº¡o:**
```typescript
book_id â†’ LÆ°u vÃ o workProfile.bookId
```

**Flow 2 sá»­ dá»¥ng:**
```typescript
workProfile.bookId â†’ getContextForQuery(bookId, query)
```

---

## ğŸ“ Files Modified

### **Backend**
1. `server/db/schema.sql` - Changed `vector(768)` â†’ `vector(384)`
2. `server/routes/googleDocs.ts` - Database init & job queueing
3. `server/jobs/chapterProcessingJob.ts` - Model version: `'all-MiniLM-L6-v2'`
4. `server/services/semanticSearchService.ts` - Updated SQL queries vá»›i `vector(384)`

### **Frontend**
5. `types.ts` - Added `bookId` fields
6. `App.tsx` - bookId handling
7. `services/geminiService.ts` - Database context retrieval

### **Configuration**
8. `package.json` - Added scripts: `embedding:start`, `embedding:test`, `dev:all`

---

## âœ… Testing Checklist

### **Local Embedding Server**
- [ ] Python 3.8+ installed
- [ ] Dependencies installed (`pip install sentence-transformers fastapi uvicorn`)
- [ ] Server started (`python local_embedding_server.py`)
- [ ] Health check passed (`curl http://localhost:8000/`)
- [ ] Embedding test passed (`curl -X POST http://localhost:8000/embed ...`)

### **Database**
- [ ] Schema has `vector(384)` columns
- [ ] Migration run (if needed): `psql -U postgres -d bot_writing_advanced -f server/db/migrate_to_384.sql`
- [ ] Embeddings can be saved
- [ ] Vector search works

### **Flow 1: Import**
- [ ] Google Doc import works
- [ ] Book created in database
- [ ] Chapters created in database
- [ ] Processing jobs queued
- [ ] Embeddings generated (384 dimensions)
- [ ] Embeddings saved to database

### **Flow 2: Chat**
- [ ] `bookId` passed in `documentContext`
- [ ] Context retrieved from database
- [ ] Semantic search works
- [ ] AI response includes database context

---

## ğŸ› Troubleshooting

### **Lá»—i: Module not found**
```bash
pip install sentence-transformers fastapi uvicorn
```

### **Lá»—i: Port already in use**
```bash
# Windows:
netstat -ano | findstr :8000

# Kill process hoáº·c Ä‘á»•i port:
export PORT=8001
python local_embedding_server.py
```

**Update `.env`:**
```env
LOCAL_EMBEDDING_API_URL=http://localhost:8001
```

### **Lá»—i: Model download failed**
```bash
python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')"
```

### **Lá»—i: Out of memory**
DÃ¹ng model nháº¹ hÆ¡n:
```bash
export EMBEDDING_MODEL=paraphrase-MiniLM-L3-v2
python local_embedding_server.py
```

**Update `.env`:**
```env
LOCAL_EMBEDDING_DIMENSIONS=256
```

**Update database schema:**
```sql
ALTER TABLE recent_chapters ALTER COLUMN embedding_vector TYPE vector(256);
ALTER TABLE chapter_chunks ALTER COLUMN chunk_embedding TYPE vector(256);
ALTER TABLE embedding_cache ALTER COLUMN embedding_vector TYPE vector(256);
```

### **Lá»—i: Database vector dimension mismatch**
```bash
# Run migration script
psql -U postgres -d bot_writing_advanced -f server/db/migrate_to_384.sql
```

---

## ğŸ“Š Performance

### **Embedding Generation**
- Single embedding: ~10-50ms (CPU)
- Batch (10 texts): ~50-200ms (CPU)
- Throughput: ~100-200 embeddings/second

### **Memory Usage**
- Model: ~200-500MB RAM
- Server: ~100-200MB RAM
- Total: ~300-700MB RAM

### **Vector Search**
- Query latency: < 500ms (with index)
- Cache hit rate: > 80% (after warmup)

---

## ğŸ¯ Key Features

### **Local Embedding Provider**
- âœ… Model: `all-MiniLM-L6-v2` (384 dimensions)
- âœ… Cost: FREE (100% local)
- âœ… Privacy: 100% (no data sent to external APIs)
- âœ… Performance: ~85% of OpenAI embeddings quality
- âœ… Speed: ~100-200 embeddings/second (CPU)

### **Database Integration**
- âœ… PostgreSQL + pgvector with `vector(384)`
- âœ… Chapter-level embeddings
- âœ… Chunk-level embeddings (for long chapters)
- âœ… Embedding cache (avoid regeneration)

### **Semantic Search**
- âœ… Vector search with cosine similarity
- âœ… Hierarchical search (chapter â†’ chunk)
- âœ… Context retrieval for AI responses

---

## ğŸ¯ TÃ³m Táº¯t

1. **Local Embedding Provider:** `all-MiniLM-L6-v2` (384 dimensions, free)
2. **Flow 1:** Import â†’ Process â†’ Generate embeddings â†’ Save to database
3. **Flow 2:** Query â†’ Semantic search â†’ Get context â†’ AI response
4. **Database:** PostgreSQL + pgvector vá»›i `vector(384)`
5. **Architecture:** Free-first, pluggable, cÃ³ thá»ƒ switch provider sau

---

**Status:** âœ… Implementation Complete  
**Ready for:** Testing & Validation


